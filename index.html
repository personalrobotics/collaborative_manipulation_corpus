<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Collaborative manipulation corpus by personalrobotics</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Collaborative manipulation corpus</h1>
      <h2 class="project-tagline">A Corpus of Natural Language Instructions for Collaborative Manipulation</h2>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus" class="btn">View on GitHub</a>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction.</h2>

<p>This site presents a dataset of natural language instructions for object specification in manipulation scenarios. It is comprised of 1607 individual written instructions which were collected via online crowdsourcing. This dataset is particularly useful for researchers who work in natural language processing, human-robot interaction, and robotic tabletop manipulation. It provides a benchmark of image/instruction pairs to be used in system evaluations as well as uncovers inherent challenges in table-top object specification. </p>

<h2>
<a id="the-dataset-is-published-in-international-journal-of-robotics-research" class="anchor" href="#the-dataset-is-published-in-international-journal-of-robotics-research" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The dataset is published in International Journal of Robotics Research</h2>

<p>R. Scalise<em>, S. Li</em>, H. Admoni, S. Rosenthal, and S. Srinivasa <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/ijrr_2016.pdf">"Placeholder title for Natural Language Instructions for Collaborative Manipulation,"</a> <em>International Journal of Robotics Research</em>, in press. </p>

<h2>
<a id="example-code-is-provided-for-easy-access-via-python" class="anchor" href="#example-code-is-provided-for-easy-access-via-python" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example code is provided for easy access via Python</h2>

<p>!!!!!!!!!!!!!!!!!!need to be updated</p>

<h2>
<a id="datasheet-in-csv-format" class="anchor" href="#datasheet-in-csv-format" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Datasheet in CSV format</h2>

<ol>
<li><p><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/data/forward_study_per_sentence_user.csv">Study 1 Dataset</a></p></li>
<li><p><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/data/reverse_study_per_sentence_user.csv">Study 2 Dataset</a></p></li>
</ol>

<h2>
<a id="images" class="anchor" href="#images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Images</h2>

<ol>
<li>
<p><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/master/study1_images_with_red_circles">The images used in Study 1</a></p>

<ul>
<li><p>"Configuration_example_page.png" is particularly used in the example page of the online Mechanical Turk study.</p></li>
<li><p>Images from "Configuration_01_**.png" to "Configuration_14_**.png" are the images used as actual stimuli. From each of the 14 configurations, there are 2 possible target blocks selected which are indicated by an arrow ("Configuration_**<em>v1.png" and "Configuration</em>**_v2.png"). Therefore, in total, there are 28 unique scenarios in the set of stimulus.</p></li>
</ul>
</li>
<li>
<p><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/master/study2_images_without_red_circles">The images used in Study 2</a></p>

<ul>
<li>It contains 1 image particularly used in the example page and 14 different images as the actual stimuli.</li>
</ul>
</li>
</ol>

<h2>
<a id="publications-based-on-this-dataset" class="anchor" href="#publications-based-on-this-dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publications based on this dataset</h2>

<h3>
<a id="conference-papers" class="anchor" href="#conference-papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conference Papers</h3>

<p>Shen Li<em>, Rosario Scalise</em>, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/roman_conf_2016.pdf">Spatial references and perspective in natural language instructions for collaborative manipulation.</a> <em>In Proceedings of the International Symposium on Robot and Human Interactive Communication Conference.</em> IEEE, 2016.</p>

<h3>
<a id="workshop-papers" class="anchor" href="#workshop-papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Workshop Papers</h3>

<p>Shen Li<em>, Rosario Scalise</em>, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/rss_ws_2016.pdf">Perspective in Natural Language Instructions for Collaborative Manipulation.</a> <em>In Proceedings of the Robotics: Science and Systems Workshop on Model Learning for Human-Robot Communication.</em> 2016.</p>

<h3>
<a id="posters" class="anchor" href="#posters" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Posters</h3>

<p><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/rss_poster_2016.pdf">Workshop at Robotics: Science and Systems 2016 - Model Learning for Human-Robot Communication</a></p>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

<p>If you have any questions about the dataset, or intend to collaborate with us on human-robot communication, please contact us! We are open and excited to collaborate!</p>

<p>You can reach either of us via email:</p>

<p>Rosario Scalise <a href="mailto:robo@cmu.edu">robo@cmu.edu</a></p>

<p>Shen Li <a href="mailto:shenli@cmu.edu">shenli@cmu.edu</a></p>

<p><img src="https://raw.githubusercontent.com/personalrobotics/collaborative_manipulation_corpus/master/blockStudy.png?token=AG4N80hByyT4ztbT3U2FgnUxdoDwUZ14ks5XjoaUwA%3D%3D" alt="corpus_keywords"></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus">Collaborative manipulation corpus</a> is maintained by <a href="https://github.com/personalrobotics">personalrobotics</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
