<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Collaborative Manipulation Corpus by personalrobotics</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Collaborative Manipulation Corpus</h1>
      <h2 class="project-tagline">A Corpus of Natural Language Instructions for Human-Robot Collaborative Manipulation</h2>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus" class="btn">View on GitHub</a>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

<p>This web page supports a dataset of natural language instructions for object specification in manipulation scenarios. It is comprised of 1607 individual written instructions which were collected via online crowdsourcing. This dataset is particularly useful for researchers who work in natural language processing, human-robot interaction, and robotic tabletop manipulation. In addition to serving as a rich corpus of domain specific language, it provides a benchmark of image/instruction pairs to be used in system evaluations as well as uncovers inherent challenges in tabletop object specification. </p>

<h3>
<a id="referred-journal-publication" class="anchor" href="#referred-journal-publication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Referred Journal Publication</h3>

<p>R. Scalise*, S. Li*, H. Admoni, S. Rosenthal, and S. Srinivasa <a href="./docs/ijrr_2016.pdf">"Natural Language Instructions for Human-Robot Collaborative Manipulation"</a>, <em>International Journal of Robotics Research</em>, in press.</p>

<h3>
<a id="data-and-accessing-code" class="anchor" href="#data-and-accessing-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data and accessing code</h3>

<ol>
<li>
<p>Primary Dataset: Natural Language Instructions Corpus</p>

<ul>
<li><p><a href="./data/forward_study_per_sentence_user.csv">Data in CSV</a></p></li>
<li><p><a href="./code/access_forward_study_data_CSV.py">Accessing code in Python</a></p></li>
</ul>
</li>
<li>
<p>Supplementary Dataset: Instruction Evaluation</p>

<ul>
<li><p><a href="./data/reverse_study_per_sentence_user.json">Data in JSON</a></p></li>
<li>
<p><a href="./code/access_reverse_study_data_JSON.py">Accessing code in Python</a></p>

<ul>
<li>Note: in accessing code of study 2, <code>r_target_block_index</code> is referring to the index of the target block. The index of all the blocks and the indices of target blocks in both versions of each scenario on the tabletop are annotated in <a href="./docs/images_code.pdf">images_code.pdf</a>
</li>
</ul>
</li>
</ul>
</li>
</ol>

<h3>
<a id="stimulus-images" class="anchor" href="#stimulus-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Stimulus Images</h3>

<ol>
<li>
<p><a href="./study1_images_with_red_arrows">The stimulus images used in Study 1</a></p>

<ul>
<li><p>"Configuration_example_page.png" is specifically used in the example page of the online Mechanical Turk study.</p></li>
<li><p>Images from "Configuration_01_**.png" to "Configuration_14_**.png" are the images used as actual stimuli. From each of the 14 configurations, there are 2 possible target blocks selected which are indicated by a red arrow ("Configuration_**_v1.png" and "Configuration_**_v2.png"). In total, there are 28 unique scenarios in the set of stimulus.</p></li>
<li>
<p>An example   </p>

<p align="center"><img alt="stimulus_image_example_1" src="./study1_images_with_red_arrows/Configuration_example_page.png" width="480"></p>
</li>
</ul>
</li>
<li>
<p><a href="./study2_images_without_red_arrows">The stimulus images used in Study 2</a></p>

<ul>
<li><p>It contains 1 image specifically used in the example page and 14 different images as the actual stimuli.</p></li>
<li>
<p>An example</p>

<p align="center"><img alt="stimulus_image_example_2" src="./study2_images_without_red_arrows/Configuration_example_page.png" width="480" align="center"></p>
</li>
</ul>
</li>
</ol>

<h3>
<a id="publications-based-on-this-dataset" class="anchor" href="#publications-based-on-this-dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publications based on this dataset</h3>

<ol>
<li>
<p>Conference Papers</p>

<p>Shen Li*, Rosario Scalise*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. <a href="./docs/roman_conf_2016.pdf">Spatial references and perspective in natural language instructions for collaborative manipulation.</a> <em>In Proceedings of the International Symposium on Robot and Human Interactive Communication Conference.</em> IEEE, 2016.</p>
</li>
<li>
<p>Workshop Papers</p>

<p>Shen Li*, Rosario Scalise*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. <a href="./docs/rss_ws_2016.pdf">Perspective in Natural Language Instructions for Collaborative Manipulation.</a> <em>In Proceedings of the Robotics: Science and Systems Workshop on Model Learning for Human-Robot Communication.</em> 2016.</p>
</li>
<li>
<p>Posters</p>

<p><a href="./docs/rss_poster_2016.pdf">Workshop at Robotics: Science and Systems 2016 - Model Learning for Human-Robot Communication</a></p>
</li>
</ol>

<h3>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h3>

<p>If you have any questions about the dataset, or intend to collaborate with us on human-robot communication, please contact us! We are open and excited to collaborate!</p>

<p>You can reach either of us via email:</p>

<p>Rosario Scalise <a href="mailto:rscalise@andrew.cmu.edu">rscalise@andrew.cmu.edu</a></p>

<p>Shen Li <a href="mailto:shenli@cmu.edu">shenli@cmu.edu</a></p>

<p><img src="./images/blockStudy.png" alt="corpus_keywords">.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus">Collaborative Manipulation Corpus</a> is maintained by <a href="https://github.com/personalrobotics">personalrobotics</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
