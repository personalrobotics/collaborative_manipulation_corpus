<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Collaborative Manipulation Corpus by personalrobotics</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <style>
        /* SCROLL TABLE ESSENTIALS (+ SOME ADDITIONAL CSS): */
        div#scrollTableContainer {
            width: 617px;
            margin: 40px; /* just for presentation purposes */
            border: 1px solid black;
        }
        #tHeadContainer {
        background: #CC3600;
        color: white;
        font-weight: bold;
        }
        #tBodyContainer {
            height: 240px;
            overflow-y: scroll;
        }
    </style>
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Collaborative Manipulation Corpus</h1>
      <h2 class="project-tagline">A Corpus of Natural Language Instructions for Human-Robot Collaborative Manipulation</h2>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus" class="btn" target="_blank">View on GitHub</a>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/zipball/master" class="btn" target="_blank">Download .zip</a>
      <a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/tarball/master" class="btn"target="_blank">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h3>

<p>This web page supports a dataset of natural language instructions for object specification in manipulation scenarios. It is comprised of 1582 individual written instructions which were collected via online crowdsourcing. This dataset is particularly useful for researchers who work in natural language processing, human-robot interaction, and robotic tabletop manipulation. In addition to serving as a rich corpus of domain specific language, it provides a benchmark of image/instruction pairs to be used in system evaluations as well as uncovers inherent challenges in tabletop object specification. </p>

<h3>
<a id="referred-journal-publication" class="anchor" href="#referred-journal-publication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Referred Journal Publication</h3>

<p>R. Scalise*, S. Li*, H. Admoni, S. Rosenthal, and S. Srinivasa <a href="./docs/ijrr_2016.pdf" target="_blank">"Natural Language Instructions for Human-Robot Collaborative Manipulation"</a>, <em>International Journal of Robotics Research</em>, in press.</p>

<h3>
<a id="data-and-accessing-code" class="anchor" href="#data-and-accessing-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data and accessing code</h3>

<ol>
<li>
<p>Primary Dataset: Natural Language Instructions Corpus</p>

<ul>
<li><p><a href="./data/NLICorpusData.csv" target="_blank">Data in CSV</a></p></li>
<li><p><a href="./data/NLICorpusData_1400.csv" target="_blank">Downsampled Data in CSV</a></p>
<p>(We downsampled the data from 1582 to 1400 and use the 1400 instructions in the evaluation study)</p></li>
<li><p><a href="./code/access_NLICorpusData_CSV.py" target="_blank">Accessing code in Python</a></p></li>
<li><p>Data example</p></li>
<table>
  <tr>
    <th>Description</th>
    <th>Index</th>
    <th>Scenario</th>
    <!-- <th>AgentType</th>
    <th>Difficulty</th>
    <th>TimeToComplete</th>
    <th>...</th> -->
  </tr>
  <tr>
    <td>Pick up the yellow cube.</td>
    <td>1341</td>
    <td>Configuration_1_v1.png</td>
    <!-- <td>human</td>
    <td>5</td>
    <td>0:00:16</td>
    <td>...</td> -->
  </tr>
  <tr>
    <!-- <th>Description</th>
    <th>Index</th>
    <th>Scenario</th> -->
    <th>AgentType</th>
    <th>Difficulty</th>
    <th>TimeToComplete</th>
    <td>...</td>
  </tr>
  <tr>
    <!-- <td>Pick up the yellow cube.</td>
    <td>1341</td>
    <td>Configuration_1_v1.png</td> -->
    <td>human</td>
    <td>5</td>
    <td>0:00:16</td>
    <td>...</td>
  </tr>
</table>
<ul>
  <li><p>Description or Instruction: The participant gave it to describe the indicated block</p></li>
  <li><p>Index: This is the No. 1341 description</p></li>
  <li><p>Scenario: In study 1, we show people either Configuration_01_v1.png or Configuration_01_v2.png. Here it is Configuration_01_v1.png, which you can access at <a href="./study1_images_with_red_arrows/Configuration_01_v1.png" target="_blank">Configuration_01_v1.png</a></p></li>
  <li><p>AgentType: The participant is instructing a <i>human</i> instead of a <i>robot</i></p></li>
  <li><p>Difficulty: The participant rated this scenario as 5 (most difficult)</p></li>
  <li><p>TimeToComplete: The participant spent 16 seconds to generate this description</p></li>
</ul>
</ul>
</li>
<li>
<p>Supplementary Dataset: Instruction Evaluation</p>

<ul>
<li><p><a href="./data/evaluationData.json" target="_blank">Full Data in JSON</a></p></li>
<li><p><a href="./data/evaluationData.csv" target="_blank">Full Data in CSV</a></p></li>
<li><p><a href="./data/evaluationDataAvg.json" target="_blank">Average Data in JSON</a></p></li>
<li><p><a href="./data/evaluationDataAvg.csv" target="_blank">Average Data in CSV</a></p></li>
<li><p><a href="./code/access_evaluationData_JSON.py" target="_blank">Python code to access JSON data</a></p></li>
<li><p><a href="./code/access_evaluationData_CSV.py" target="_blank">Python code to access CSV data</a></p></li>
<ul>
<li>Note: in accessing code of study 2, <code>r_target_block_index</code> is referring to the index of the target block. The index of all the blocks and the indices of target blocks in both versions of each scenario on the tabletop are annotated in <a href="./docs/images_code.pdf" target="_blank">images_code.pdf</a>
</li>
</ul>
<br>
<div style="height:250px; width:750px; overflow-x:scroll ; padding-bottom:10px;">
    <table>
      <tr>
        <th>Instruction</th>
        <th>Index</th>
        <th>Scenario</th>
        <th>NumOfWords</th>
        <th>TargetBlockId</th>
        <th>ClickedBlockId</th>
        <th>Correctness</th> 
        <th>TimeToComplete</th>
        <th>...</th>
      </tr>
      <tr>
        <td>Pick up the yellow cube.</td>
        <td>1341</td>
        <td>Configuration_1_v1.png</td>
        <td>5</td>
        <td>1</td>
        <td>1</td>
        <td>1</td>
        <td>3.593606</td>
        <td>...</td>
      </tr>
    </table>
</div>
<br>
<ul>
  <li><p>Description or Instruction: The participant looked for the indicated block based on this instruction</p></li>
  <li><p>Index: This is the No. 1341 description</p></li>
  <li><p>Scenario: In study 2, we don't distinguish Configuration_01_v1.png or Configuration_01_v2.png. So you can access this image at <a href="./study2_images_without_red_arrows/Configuration_01.png" target="_blank">Configuration_01.png</a></p></li>
  <li><p>NumOfWords: Number of words within this instruction</p></li>
  <li><p>TargetBlockId: You can find out the target block by referring to <a href="./docs/images_code.pdf" target="_blank">images_code.pdf</a> Page 1. You can find the block indicated by '1'</p></li>
  <li><p>ClickedBlockId: You can find out the block selected by the participant by referring to <a href="./docs/images_code.pdf" target="_blank">images_code.pdf</a> Page 1. You can find the block indicated by '1'</p></li>
  <li><p>Correctness: The participant successfully select the target block (1 if right, 0 if wrong)</p></li>
  <li><p>TimeToComplete: The participant spent 3.593606 seconds to generate this description</p></li>
</ul>
</li>
</ul>
</li>
</ol>

<h3>
<a id="stimulus-images" class="anchor" href="#stimulus-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Stimulus Images</h3>

<ol>
<li>
<p><a href="./study1_images_with_red_arrows" target="_blank">The stimulus images used in Study 1</a></p>

<ul>
<li><p>"Configuration_example_page.png" is specifically used in the example page of the online Mechanical Turk study.</p></li>
<li><p>Images from "Configuration_01_**.png" to "Configuration_14_**.png" are the images used as actual stimuli. From each of the 14 configurations, there are 2 possible target blocks selected which are indicated by a red arrow ("Configuration_**_v1.png" and "Configuration_**_v2.png"). In total, there are 28 unique scenarios in the set of stimulus.</p></li>
<li>
<p>An example   </p>

<p align="center"><img alt="stimulus_image_example_1" src="./study1_images_with_red_arrows/Configuration_example_page.png" width="480"></p>
</li>
</ul>
</li>
<li>
<p><a href="./study2_images_without_red_arrows" target="_blank">The stimulus images used in Study 2</a></p>

<ul>
<li><p>It contains 1 image specifically used in the example page and 14 different images as the actual stimuli.</p></li>
<li>
<p>An example</p>

<p align="center"><img alt="stimulus_image_example_2" src="./study2_images_without_red_arrows/Configuration_example_page.png" width="480" align="center"></p>
</li>
</ul>
</li>
</ol>

<h3>
<a id="publications-based-on-this-dataset" class="anchor" href="#publications-based-on-this-dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publications based on this dataset</h3>

<ol>
<li>
<p>Conference Papers</p>

<p>Shen Li*, Rosario Scalise*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. <a href="./docs/roman_conf_2016.pdf" target="_blank">Spatial references and perspective in natural language instructions for collaborative manipulation.</a> <em>In Proceedings of the International Symposium on Robot and Human Interactive Communication Conference.</em> IEEE, 2016.</p>
</li>
<li>
<p>Workshop Papers</p>

<p>Shen Li*, Rosario Scalise*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. <a href="./docs/rss_ws_2016.pdf" target="_blank">Perspective in Natural Language Instructions for Collaborative Manipulation.</a> <em>In Proceedings of the Robotics: Science and Systems Workshop on Model Learning for Human-Robot Communication.</em> 2016.</p>
</li>
<li>
<p>Posters</p>

<p><a href="./docs/rss_poster_2016.pdf" target="_blank">Workshop at Robotics: Science and Systems 2016 - Model Learning for Human-Robot Communication</a></p>
</li>
</ol>

<h3>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h3>

<p>If you have any questions about the dataset, or intend to collaborate with us on human-robot communication, please contact us! We are open and excited to collaborate!</p>

<p>You can reach either of us via email:</p>

<p>Rosario Scalise <a href="mailto:rscalise@andrew.cmu.edu">rscalise@andrew.cmu.edu</a></p>

<p>Shen Li <a href="mailto:shenli@cmu.edu">shenli@cmu.edu</a></p>


<h3>
<a id="license" class="anchor" href="#license" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>License</h3>
    <p>Source code is provided under the 
        <a href="http://opensource.org/licenses/MIT" target="_blank">MIT License</a> and the .csv  data is available 
       under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> license. 
    </p>     

<p align="center"><img alt="corpus words" src="./images/blockStudy.png" width="800" align="center"></p>

      <footer class="site-footer">
  
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        <span class="site-footer-owner"><a href="https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/gh-pages"target="_blank">Collaborative Manipulation Corpus</a> is maintained by <a href="https://github.com/personalrobotics" target="_blank">personalrobotics</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com" target="_blank">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme" target="_blank">Cayman theme</a> by <a href="https://twitter.com/jasonlong" target="_blank">Jason Long</a>.</span>
      </footer>
    </section>

  
  </body>
</html>
