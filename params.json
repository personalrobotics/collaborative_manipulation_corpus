{
  "name": "Collaborative Manipulation Corpus",
  "tagline": "A Corpus of Natural Language Instructions for Human-Robot Collaborative Manipulation",
  "body": "### Introduction\r\nThis web page supports a dataset of natural language instructions for object specification in manipulation scenarios. It is comprised of 1582 individual written instructions which were collected via online crowdsourcing. This dataset is particularly useful for researchers who work in natural language processing, human-robot interaction, and robotic tabletop manipulation. In addition to serving as a rich corpus of domain specific language, it provides a benchmark of image/instruction pairs to be used in system evaluations as well as uncovers inherent challenges in tabletop object specification. \r\n\r\n### Referred Journal Publication\r\nR. Scalise\\*, S. Li\\*, H. Admoni, S. Rosenthal, and S. Srinivasa [\"Natural Language Instructions for Human-Robot Collaborative Manipulation\"](https://github.com/personalrobotics/collaborative\\_manipulation\\_corpus/blob/master/docs/ijrr\\_2016.pdf), *International Journal of Robotics Research*, in press.\r\n\r\n### Data and accessing code\r\n1. Primary Dataset: Natural Language Instructions Corpus\r\n\r\n    * [Data in CSV](https://github.com/personalrobotics/collaborative\\_manipulation\\_corpus/blob/master/data/forward\\_study\\_per\\_sentence\\_user.csv)\r\n\r\n    * [Accessing code in Python](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/code/access_forward_study_data_CSV.py)\r\n\r\n2. Supplementary Dataset: Instruction Evaluation\r\n\r\n    * [Data in JSON](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/data/reverse_study_per_sentence_user.json)\r\n\r\n    * [Accessing code in Python](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/code/access_reverse_study_data_JSON.py)\r\n\r\n        * Note: in accessing code of study 2, ```r_target_block_index``` is referring to the index of the target block. The index of all the blocks and the indices of target blocks in both versions of each scenario on the tabletop are annotated in [images_code.pdf](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/images_code.pdf)\r\n\r\n\r\n### Stimulus Images\r\n1. [The stimulus images used in Study 1](https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/master/study1_images_with_red_arrows)\r\n\r\n    * \"Configuration\\_example\\_page.png\" is specifically used in the example page of the online Mechanical Turk study.\r\n\r\n    * Images from \"Configuration\\_01\\_\\*\\*.png\" to \"Configuration\\_14\\_\\*\\*.png\" are the images used as actual stimuli. From each of the 14 configurations, there are 2 possible target blocks selected which are indicated by a red arrow (\"Configuration\\_\\*\\*\\_v1.png\" and \"Configuration\\_\\*\\*\\_v2.png\"). In total, there are 28 unique scenarios in the set of stimulus.\r\n\r\n    * An example   \r\n    <p align=\"center\"><img alt=\"stimulus_image_example_1\" src=\"https://raw.githubusercontent.com/personalrobotics/collaborative_manipulation_corpus/master/study1_images_with_red_arrows/Configuration_example_page.png?token=AG4N889oHe6nXt3ONmEkdHHiXEvxNPNYks5Xqq8VwA%3D%3D\" width=\"480\"></p>\r\n\r\n2. [The stimulus images used in Study 2](https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/master/study2_images_without_red_arrows)\r\n\r\n    * It contains 1 image specifically used in the example page and 14 different images as the actual stimuli.\r\n    \r\n    * An example\r\n    <p align=\"center\"><img alt=\"stimulus_image_example_2\" src=\"https://raw.githubusercontent.com/personalrobotics/collaborative_manipulation_corpus/master/study2_images_without_red_arrows/Configuration_example_page.png?token=AG4N8828Ho8csfwwHbymc71WXhGjJjIXks5Xqq8wwA%3D%3D\" width=\"480\" align=\"center\"></p>\r\n\r\n### Publications based on this dataset\r\n\r\n1. Conference Papers\r\n\r\n    Shen Li\\*, Rosario Scalise\\*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. [Spatial references and perspective in natural language instructions for collaborative manipulation.](https://github.com/personalrobotics/collaborative\\_manipulation\\_corpus/blob/master/docs/roman\\_conf\\_2016.pdf) *In Proceedings of the International Symposium on Robot and Human Interactive Communication Conference.* IEEE, 2016.\r\n\r\n2. Workshop Papers\r\n\r\n    Shen Li\\*, Rosario Scalise\\*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. [Perspective in Natural Language Instructions for Collaborative Manipulation.](https://github.com/personalrobotics/collaborative\\_manipulation\\_corpus/blob/master/docs/rss\\_ws\\_2016.pdf) *In Proceedings of the Robotics: Science and Systems Workshop on Model Learning for Human-Robot Communication.* 2016.\r\n\r\n3. Posters\r\n\r\n    [Workshop at Robotics: Science and Systems 2016 - Model Learning for Human-Robot Communication](https://github.com/personalrobotics/collaborative\\_manipulation\\_corpus/blob/master/docs/rss\\_poster\\_2016.pdf)\r\n\r\n### Contact\r\nIf you have any questions about the dataset, or intend to collaborate with us on human-robot communication, please contact us! We are open and excited to collaborate!\r\n\r\nYou can reach either of us via email:\r\n\r\nRosario Scalise rscalise@andrew.cmu.edu\r\n\r\nShen Li shenli@cmu.edu\r\n\r\n<p align=\"center\"><img alt=\"corpus words\" src=\"https://raw.githubusercontent.com/personalrobotics/collaborative_manipulation_corpus/gh-pages/images/blockStudy.png?token=AOJzv7Gb7BAsGzxtzPBnjqtBktvIpC95ks5XrFDawA%3D%3D\" width=\"480\" align=\"center\"></p>",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}