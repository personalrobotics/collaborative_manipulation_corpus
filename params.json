{
  "name": "Collaborative manipulation corpus",
  "tagline": "A Corpus of Natural Language Instructions for Collaborative Manipulation",
  "body": "## Introduction.\r\nThis site presents a dataset of natural language instructions for object specification in manipulation scenarios. It is comprised of 1607 individual written instructions which were collected via online crowdsourcing. This dataset is particularly useful for researchers who work in natural language processing, human-robot interaction, and robotic tabletop manipulation. It provides a benchmark of image/instruction pairs to be used in system evaluations as well as uncovers inherent challenges in table-top object specification. \r\n\r\n## The dataset is published in International Journal of Robotics Research\r\nR. Scalise*, S. Li*, H. Admoni, S. Rosenthal, and S. Srinivasa [``Placeholder title for Natural Language Instructions for Collaborative Manipulation,''](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/ijrr_2016.pdf) *International Journal of Robotics Research*, in press. \r\n\r\n## Example code is provided for easy access via Python\r\n!!!!!!!!!!!!!!!!!!need to be updated\r\n\r\n## Datasheet in CSV format\r\n[Study 1 Dataset](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/data/forward_study_per_sentence_user.csv)\r\n\r\n[Study 2 Dataset](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/data/reverse_study_per_sentence_user.csv)\r\n\r\n## Images\r\n[The images used in Study 1] (https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/master/study1_images_with_red_circles)\r\n1``Configuration_example_page.png'' is particularly used in the example page of the online Mechanical Turk study.\r\n2 Images from ``Configuration_01_**.png'' to ``Configuration_14_**.png'' are the images used as actual stimuli. From each of the 14 configurations, there are 2 possible target blocks selected which are indicated by an arrow (``Configuration_**_v1.png'' and ``Configuration_**_v2.png''). Therefore, in total, there are 28 unique scenarios in the set of stimulus.\r\n\r\n[The images used in Study 2](https://github.com/personalrobotics/collaborative_manipulation_corpus/tree/master/study2_images_without_red_circles)\r\nIt contains 1 image particularly used in the example page and 14 different images as the actual stimuli.\r\n\r\n\r\n## Publications based on this dataset\r\n### Conference Papers\r\nShen Li*, Rosario Scalise*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. [Spatial references and perspective in natural language instructions for collaborative manipulation.](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/roman_conf_2016.pdf) *In Proceedings of the International Symposium on Robot and Human Interactive Communication Conference.* IEEE, 2016.\r\n\r\n### Workshop Papers\r\nShen Li*, Rosario Scalise*, Henny Admoni, Stephanie Rosenthal, and Siddhartha S Srinivasa. [Perspective in Natural Language Instructions for Collaborative Manipulation.](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/rss_ws_2016.pdf) *In Proceedings of the Robotics: Science and Systems Workshop on Model Learning for Human-Robot Communication.* 2016.\r\n\r\n### Posters\r\n[Workshop at Robotics: Science and Systems 2016 - Model Learning for Human-Robot Communication](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/docs/rss_poster_2016.pdf)\r\n\r\n\r\n\r\n## Contact\r\nIf you have any questions about the dataset, or intend to collaborate with us on human-robot communication, please contact us! We are open and excited to collaborate!\r\n\r\nYou can reach either of us via email:\r\nRosario Scalise robo@cmu.edu\r\nShen Li shenli@cmu.edu\r\n\r\n![corpus_keywords](https://github.com/personalrobotics/collaborative_manipulation_corpus/blob/master/blockStudy.pdf)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}